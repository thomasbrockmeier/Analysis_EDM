Preliminary analysis
========================================================
Timbre similarity
-------------------------------------------------------

### Data cleanup

```{r echo=FALSE}
timbre <- read.delim("timbre20140606 (no dubious responses).csv")
source("reliability.check.R")
source("clean.data.R")
source("descriptives.R")
source("zscores.R")
source("fleiss.EDM.R")
source("icc.EDM.R")
rtim <- reliability.check(database=timbre)
descrm1no <- descriptives(rtim, conf.1.rm=F)
descrm1yes <- descriptives(rtim, conf.1.rm=T)
zstim <- zscores(data=rtim)
```
As a first step into the analysis of the data obtained via the Survey Gizmo (*n* = `r nrow(timbre)`) online
platform, we needed to clean up the data. To achieve this, I followed a number of
steps:

1. A function ([reliability.check.R](https://github.com/dilopez1/Cleaning_scripts_R/blob/master/reliability.check.R))
compares the repeated elements at the beginning and end of the experiment for each participant, which
should be equal (within a Â±1 margin). This leaves us with `r nrow(rtim)` cases.
2. A script ([clean.data.R](https://github.com/dilopez1/Cleaning_scripts_R/blob/master/clean.data.R)) extracts only the ratings given for each pair of segments. This function has the option to remove the ratings where the rater was "Not confident"
(confidence level 1) about his/her answer, or get the data as-is. When confidence level 1
scores are taken into account, the least number of ratings a pair has is `r min(descrm1no$n)`
and the largest number is `r max(descrm1no$n)`. On the other hand, when confidence level
1 scores are removed, the number of ratings ranges between `r min(descrm1yes$n)` and
`r max(descrm1yes$n)`.
3. Another database was generated by polarizing the 4-point based ratings into
"Dissimilar" (1) and "Similar" (2).

With this clean database, I ran some statistical analyses to find interesting trends.

## Fleiss' Kappa

Fleiss' Kappa assesses the reliability of agreement between a set number of raters. 
In our case, the raters were exposed to a fraction of the total amount of pairs,
hence our incomplete database. I wrote fleiss.EDM.R to pick the minimum number of ratings
at random for all pairs, and get the Kappa statistic. Since it is random, it is performed
as many times as needed by entering the number of iterations. The function outputs
the descriptive statistics of the list made from the Kappa value of each of the
iterations.

### Example
#### 4-point scale
```{r}
# The current raw dataset was loaded as "rtim"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
fexample <- fleiss.EDM(data=rtim, conf.1.rm=FALSE, polarize=FALSE, iterations=1000)
```

Here we can observe how out of the `r fexample$n` iterations, we obtained a *mean*
of **`r fexample$mean`**, a *minimum* of **`r fexample$min`**, and a *maximum* of **`r fexample$max`**.

#### 2-point scale
```{r}
# The current raw dataset was loaded as "rtim"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
fexample <- fleiss.EDM(data=rtim, conf.1.rm=FALSE, polarize=TRUE, iterations=1000)
```

Here we can observe how out of the `r fexample$n` iterations, we obtained a *mean*
of **`r fexample$mean`**, a *minimum* of **`r fexample$min`**, and a *maximum* of **`r fexample$max`**.

## Interclass Correlation Coefficient

Another measure for reliability of agreement between (and also within) raters is
the Interclass Correlation Coefficient (ICC). This measure stands apart from other
agreement measures because of its ability to analyze **exchangeable measurements**.
This means that it takes into account systematic differences among observer, thanks
to its nature as a correlation. However, unlike most correlations, ICC works not only with pairs,
but also with larger groups.

The function icc.EDM.R uses the same procedure as fleiss.EDM.R, where ratings are
chosen at random, and the the computation of the ICC statistic is made any number
of times as deemed necessary.

### Example
#### 4-point scale
```{r}
# The current raw dataset was loaded as "rtim"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
icc.EDM(data=rtim, conf.1.rm=FALSE, polarize=FALSE, iterations=1000)
```

#### 2-point scale
```{r}
# The current raw dataset was loaded as "rtim"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
icc.EDM(data=rtim, conf.1.rm=FALSE, polarize=TRUE, iterations=1000)
```

## Within Participant Concordance
Another part of the project was to investigate if subjects could rate the same
segment pairs consistently. We asked one participant to rate the same 18 segments
six times. The analysis was made with both 4-point and 2-point scales.

### 4-point scale analysis

```{r}
timbreWPC <- read.csv("timbreWPC.csv")
kappam.fleiss(ratings=t(timbreWPC[,seq(22, 57, 2)]))
icc(ratings=t(timbreWPC[,seq(22, 57, 2)]))
```

### 2-point scale analysis

```{r}
timbreWPC[timbreWPC == 2] <- 1 ; timbreWPC[timbreWPC == 3 | timbreWPC == 4] <- 2
kappam.fleiss(ratings=t(timbreWPC[,seq(22, 57, 2)]))
icc(ratings=t(timbreWPC[,seq(22, 57, 2)]))
```

Rhythm similarity
-------------------------------------------------------

For the rhythm experiment, Thomas has already analyzed the data with Fleiss' Kappa.
Here I will use the ICC function to gain some new insights on it.

```{r echo=FALSE}
rhythm <- read.delim("rc.csv")
rrhy <- reliability.check(database=rhythm)
```

#### 4-point scale
```{r}
# The current raw dataset was loaded as "rrhy"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
icc.EDM(data=rrhy, conf.1.rm=FALSE, polarize=FALSE, iterations=1000)
```

#### 2-point scale
```{r}
# The current raw dataset was loaded as "rrhy"
# The variable "conf.1.rm" stands for "confidence level 1: remove"
# The variable "polarize" changes the data from 4-point base to 2-point base.
icc.EDM(data=rrhy, conf.1.rm=FALSE, polarize=TRUE, iterations=1000)
```

As we can see, rhythm similarity shows a more promising panorama, with ICC values 
around 0.30, meaning a positive correlation of the data.